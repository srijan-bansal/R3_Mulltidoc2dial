{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "tribal-arnold",
   "metadata": {},
   "source": [
    "# SPLADE: Sparse Lexical and Expansion Model for First Stage Ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "concrete-collar",
   "metadata": {},
   "source": [
    "This notebook gives a minimal example usage of our SPLADE model. In a nutshell, SPLADE learns **sparse**, **expansion-based** query/doc representations for efficient first-stage retrieval.\n",
    "\n",
    "Sparsity is induced via a regularization applied on representations, whose strength can be adjusted; it is thus possible to control the trade-off between effectiveness and efficiency. For more details, check our papers, and don't hesitate to reach out ! \n",
    "* v1 (SIGIR21 short paper): **SPLADE: Sparse Lexical and Expansion Model for First Stage Ranking**, https://arxiv.org/abs/2107.05720\n",
    "* v2 (arxiv) new pooling + distillation: **SPLADE v2: Sparse Lexical and Expansion Model for Information Retrieval**, https://arxiv.org/abs/2109.10086\n",
    "\n",
    "We provide weights for 4 models (in the `weights` folder):\n",
    "\n",
    "| model | MRR@10 (MS MARCO dev) | recall@1000 (MS MARCO dev) | expected FLOPS | ~ avg q length | ~ avg d length | \n",
    "| --- | --- | --- | --- | --- | --- |\n",
    "| `flops_best` (**v1**) | 32.2 | 95.5 | 0.73 | 15 | 58 |\n",
    "| `flops_efficient` (**v1**) | 29.6 | 93.3 | 0.05 | 6 | 18 |\n",
    "| `splade_max` (**v2**) | 34.0 | 96.5 | 1.32 | 18 | 92 |\n",
    "| `distilsplade_max` (**v2**) | 36.8 | 97.9 | 3.82 | 25 | 232 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "heavy-negative",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForMaskedLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "continued-offense",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Splade(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, model_type_or_dir, agg=\"max\"):\n",
    "        super().__init__()\n",
    "        self.transformer = AutoModelForMaskedLM.from_pretrained(model_type_or_dir)\n",
    "        assert agg in (\"sum\", \"max\")\n",
    "        self.agg = agg\n",
    "    \n",
    "    def forward(self, **kwargs):\n",
    "        out = self.transformer(**kwargs)[\"logits\"] # output (logits) of MLM head, shape (bs, pad_len, voc_size)\n",
    "        if self.agg == \"max\":\n",
    "            values, _ = torch.max(torch.log(1 + torch.relu(out)) * kwargs[\"attention_mask\"].unsqueeze(-1), dim=1)\n",
    "            return values\n",
    "            # 0 masking also works with max because all activations are positive\n",
    "        else:\n",
    "            return torch.sum(torch.log(1 + torch.relu(out)) * kwargs[\"attention_mask\"].unsqueeze(-1), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "hundred-diana",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the dir for trained weights \n",
    "# NOTE: because between v1 and v2 we switched the pooling mechanism (better results with max), we need to prodive\n",
    "# the agg argument depending on the set of weights we want to use\n",
    "\n",
    "#### v1\n",
    "# agg = \"sum\"\n",
    "# model_type_or_dir = \"weights/flops_efficient\"\n",
    "# model_type_or_dir = \"weights/flops_best\"\n",
    "\n",
    "##### v2\n",
    "agg = \"max\"\n",
    "model_type_or_dir = \"weights/splade_max\"\n",
    "# model_type_or_dir = \"weights/distilsplade_max\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "configured-staff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading model and tokenizer\n",
    "\n",
    "model = Splade(model_type_or_dir, agg=agg)\n",
    "model.eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_type_or_dir)\n",
    "reverse_voc = {v: k for k, v in tokenizer.vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "rubber-nation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example document from MS MARCO passage collection (doc_id = 8003157)\n",
    "\n",
    "doc = \"Glass and Thermal Stress. Thermal Stress is created when one area of a glass pane gets hotter than an adjacent area. If the stress is too great then the glass will crack. The stress level at which the glass will break is governed by several factors.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "pointed-quick",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of actual dimensions:  78\n",
      "SPLADE BOW rep:\n",
      " [('glass', 14.96), ('stress', 14.23), ('cause', 9.77), ('thermal', 6.1), ('stressed', 5.82), ('window', 5.46), ('glasses', 4.68), ('crack', 3.99), ('happen', 3.44), ('why', 3.42), ('material', 3.34), ('break', 2.36), ('shatter', 2.27), ('meaning', 2.24), ('materials', 1.98), ('heat', 1.95), ('caused', 1.74), ('do', 1.49), ('pan', 1.43), ('when', 1.39), ('strike', 1.37), ('too', 1.2), ('create', 1.18), ('it', 1.12), ('temperature', 1.09), ('created', 1.05), ('collapse', 1.04), ('generated', 1.02), ('result', 1.0), ('hot', 0.99), ('area', 0.97), ('formed', 0.92), ('fracture', 0.82), ('later', 0.8), ('factor', 0.7), ('produced', 0.69), ('hotter', 0.67), ('adjacent', 0.67), ('cooler', 0.65), ('occur', 0.65), ('determined', 0.64), ('because', 0.64), ('level', 0.63), ('difference', 0.63), ('if', 0.56), ('and', 0.53), ('than', 0.51), ('one', 0.51), ('factors', 0.51), ('pain', 0.49), ('problem', 0.49), ('related', 0.49), ('form', 0.48), ('generate', 0.44), ('at', 0.44), ('piece', 0.43), ('frame', 0.43), ('cold', 0.41), ('element', 0.41), ('surrounding', 0.38), ('##e', 0.38), ('governed', 0.36), ('pressure', 0.33), ('burst', 0.23), ('failure', 0.23), ('effect', 0.21), ('originated', 0.2), ('cracking', 0.18), ('great', 0.17), ('ceramic', 0.16), ('during', 0.13), ('condition', 0.13), ('heated', 0.11), ('crystal', 0.11), ('windows', 0.07), ('change', 0.06), ('fail', 0.04), ('metal', 0.03)]\n"
     ]
    }
   ],
   "source": [
    "# now compute the document representation\n",
    "with torch.no_grad():\n",
    "    doc_rep = model(**tokenizer(doc, return_tensors=\"pt\")).squeeze()  # (sparse) doc rep in voc space, shape (30522,)\n",
    "\n",
    "# get the number of non-zero dimensions in the rep:\n",
    "col = torch.nonzero(doc_rep).squeeze().cpu().tolist()\n",
    "print(\"number of actual dimensions: \", len(col))\n",
    "\n",
    "# now let's inspect the bow representation:\n",
    "weights = doc_rep[col].cpu().tolist()\n",
    "d = {k: v for k, v in zip(col, weights)}\n",
    "sorted_d = {k: v for k, v in sorted(d.items(), key=lambda item: item[1], reverse=True)}\n",
    "bow_rep = []\n",
    "for k, v in sorted_d.items():\n",
    "    bow_rep.append((reverse_voc[k], round(v, 2)))\n",
    "print(\"SPLADE BOW rep:\\n\", bow_rep)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
